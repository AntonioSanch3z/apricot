{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prostate cancer (PCa) is the second most frequent malignancy (after lung cancer) in men worldwide, counting $1,276,106$ new cases and causing $358,989$ deaths ($3.8\\%$ of all deaths caused by cancer in men) in $2018$ [1]. Early detection of prostate cancer allows for appropriate management of the disease, and prognostic biomarkers can help clinicians make an appropriate therapeutic decision for each patient and avoid unnecessary treatment [2].\n",
    "  \n",
    "Due to recent progress in imaging, and particularly in MRI, the so-called multi-parametric MRI (mpMRI) that combines T2-weighted imaging (T2W) with functional pulse sequences such as diffusion-weighted imaging (DWI) or dynamic contrast-enhanced (DCE) imaging has shown excellent results in PCa detection and has become the standard of care to achieve accurate and reproducible diagnosis of PCa [3,4].\n",
    "\n",
    "Pharmacokinetic modeling of the DCE-MRI signal is used to derive estimates of factors related to blood volume and permeability that are hallmarks of the angiogenic phenotype associated with most cancers. The accuracy of DCE relies on the ability to model the pharmacokinetics of an injected tracer, or contrast agent, using the signal intensity changes on sequential magnetic resonance images. \n",
    "\n",
    "The first pharmacokinetic model was proposed by Kety [5], who described flow-limited tracer uptake in tissue. This was followed by several pharmacokinetic models proposed by Tofts et al [6], Brix et al [7], and Larsson et al [8]. \n",
    "\n",
    "The majority of these models are based on the characterization of the contrast exchange rate between the plasma and the extracellular space through parameters such as $K^{trans}$, that represents the rate at which the contrast agent transfers from the blood to the interstitial space (indicating the tumor microcirculation), the reflux constant, $K_{ep}$, that reflects the rate at which the contrast agent transfers from the extravascular extracellular space back to the blood and the extravascular extracellular leakage volume fraction $v_{e}$, which predominantly reflects the percentage of contrast agent in the extravascular extracellular space.\n",
    "\n",
    "The study of these parameters helps characterize prostate cancer, so estimating them accurately and robustly is a fundamental step. These parameters are calculated using the Tofts model [9], which is equivalent to the generalized kinetic model [10],\n",
    "\n",
    "\\begin{equation}\n",
    "   \\hspace{7cm} \\frac{dC_t}{dt} = K^{trans}C_p - k_{ep}C_t     \\hspace{7cm}  (1)\n",
    "\\end{equation}\n",
    "\n",
    "where interesting parameters are $K^{trans}$, which is the transfer coefficient between blood plasma and the compartment, and the extracellular extravascular fractional volume (EES) ($v_e$). Also $k_{ep}$ is defined as $k_{ep} = K^{trans}/v_e$, and $C_t$ is the concentration of lesson tissue defined as $C_t = C_1 v_e$, where $C_1$ is the leakage space concentration. \n",
    "\n",
    "To solve equation $(1)$, we use the same approach as [11]. There, the model is restructured and expressed as a convolution as following,\n",
    "\n",
    "\\begin{equation}\n",
    "  \\hspace{4cm}  f(t) = K^{trans} \\left( a(t) \\otimes \\frac{e^{-t/T}}{T} \\right) \\equiv K^{trans} \\frac{1}{T}\\int_0^t d\\tau\\, a(\\tau) e^{-(t-\\tau)/T}  \\hspace{3cm}  (2)\n",
    "\\end{equation}\n",
    "\n",
    "where $T = 1/k_{ep}$ and $a(t)$ function is an experimental measure, so is only available at discrete times. The previous model is evaluated for values of $T \\neq 0$ by interpolating linearly between the measured values of $a(t)$. Instead, for $T = 0$ the result is $f(t) = a(t)$.\n",
    "\n",
    "Our case study consists on a prostate image with $256\\times256\\times56$ voxels and $30$ time points for each one. We have fitted this image using the equation $(2)$ implemented at the provided code in the APRICOT repository, which uses the ROOT libraries from CERN [2]. This analysis has been performed using a MPI cluster with $3$ working nodes, reducing the total computation time almost a factor $3$ compared with the same experimentation performed on a single node. The following sections will show how to reproduce this experiment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[1] P. Rawla. Epidemiology of prostate cancer. World Journal of Oncology, 10(2), 2019.  \n",
    "[2] K. M. V. T. R. P. K.-L. P. Wu X, Reinikainen P. Dynamic contrast-enhanced imaging as a prognostic tool in early diagnosis of prostate cancer: Correlation with psa and clinical stage. Contrast Media Mol Imaging, 2018.  \n",
    "[3] F. Bratan, E. Niaf, C. Melodelima, A. L. Chesnais, R. Souchon, F. Mège-Lechevallier, M. Colombel, and O. Rouvière. Influence of imaging and histological factors on prostate cancer detection and localisation on multi-parametric mri: a prospective study. European Radiology, 23(7):2019–2029, Jul 2013.  \n",
    "[4] J. D. Le, N. Tan, E. Shkolyar, D. Y. Lu, L. Kwan, L. S. Marks, J. Huang, D. J. Margolis, S. S. Raman, and R. E. Reiter. Multifocality and prostate cancer detection by multiparametric magnetic resonance imaging: Correlation with whole-mount histopathology. European Urology, 67(3):569 – 576, 2015.  \n",
    "[5] S. S. KETY. The theory and applications of the exchange of inert gas at the lungs and tissues. Pharmacological Reviews, 3(1):1–41, 1951.  \n",
    "[6] B. G. Tofts PS, Wicks DA. The mri measurement of nmr and physiological parameters in tissue to study disease process. Prog Clin Biol Res, 1991.  \n",
    "[7] Gunnar Brix;Wolfhard Semmler;Rüdiger Port;Lothar Schad;Günter Layer;Walter Lorenz. Pharmacokinetic parameters in cns gd-dtpa enhanced mr imaging. Journal of Computer Assisted Tomography, 1991.  \n",
    "[8] H. B. W. Larsson, M. Stubgaard, J. L. Frederiksen, M. Jensen, O. Henriksen, and O. B. Paulson. Quantitation of blood-brain barrier defect by magnetic resonance imaging and gadolinium-dtpa in patients with multiple sclerosis and brain tumors. Magnetic Resonance in Medicine, 16(1):117–131, 1990.  \n",
    "[9] P. S. Tofts and A. G. Kermode. Measurement of the blood-brain barrier permeability and leakage space using dynamic mr imaging. 1. fundamental concepts. Magnetic Resonance in Medicine, 17(2):357–367, 1991.  \n",
    "[10] K. M. Donahue, R. M. Weisskoff, and D. Burstein. Water diffusion and exchange as they influence contrast enhancement. Journal of Magnetic Resonance Imaging, 7(1):102–110, 1997.  \n",
    "[11] D. Flouri, D. Lesnic, and S. P. Sourbron. Fitting the two-compartment model in dce-mri by linear inversion. Magnetic Resonance in Medicine, 76(3):998–1006, 2016.  \n",
    "[12] R. Brun and F. Rademakers. Root — an object oriented data analysis framework. Nuclear Instruments and Methods in Physics Research Section A: Accelerators, Spectrometers, Detectors and Associated Equipment, 389(1):81 – 86, 1997. New Computing Techniques in Physics Research V.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Infrastructure configuration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To reproduce this experimentation, we have used a MPI cluster consisting on three nodes and one frontend. All instances have been configured with a ubuntu 16 as SO, $1$ vCPU and $4$ GB of RAM. Also, to set all working nodes up when cluster starts, set the minimum and maximum number of working nodes to the same value, in our case to $3$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the theoretical aspects of the experimentation have been discussed and the suggested infrastructure described, we can begin with the experiment itself. First of all, we need to install the local dependencies. The only requirement is the matplotlib package, which will be used to plot the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from pip._internal import main as pipmain\n",
    "pipmain([\"install\",\"--user\", \"matplotlib\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cluster configuration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, to simplify the notebook reproduction, save the deployed cluster name and SO username in the variables \"clusterName\" and \"username\" respectively. In addition, we will define a list with the working nodes where the experimentation jobs will be executed. In our case, we deployed a cluster with 3 working nodes, so, our node list is \"wn1,wn2,wn3\" or \"wn[1-3]\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "clusterName = \"perfusion\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "username = \"ubuntu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nodelist = \"wn[1-3]\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, to use the APRICOT magic functions we must load them using the %reload_ext instruction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%reload_ext apricot_magic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the module has been loaded succesfully, we could see the status of our clusters using %apricot_ls command"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%apricot_ls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dependencies installation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the cluster has been successfully configured, the next step to run our code is to install all the required dependencies at the cluster. Our code requires an external library, the root package from CERN (https://root.cern.ch). As we selected Ubuntu16 as SO for our nodes, we have been download the corresponding package version, i.e.\n",
    "\n",
    "root_v6.16.00.Linux-ubuntu16-x86_64-gcc5.4.tar.gz\n",
    "\n",
    "from https://root.cern.ch/content/release-61600\n",
    "\n",
    "For the shake of simplicity, we will suppose that the root tar file has been download at the same folder where this notebook is running. If you require a different version/distribution package, please, change the root tar name at the following variable,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rootTarName = \"root_v6.16.00.Linux-ubuntu16-x86_64-gcc5.4.tar.gz\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To install the dependencies, first we must wait untill all working nodes states have set to \"idle\". At this point, all working nodes are configured and ready to get jobs. To check their status, we can use %apricot_nodels instruction as following,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%apricot_nodels $clusterName"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once all working nodes are at idle state, the next step consists to install the required packages from the repository. Notice that the name of the packages and the install command should be changed depending on the selected SO. To install the packages on all working nodes, we use the instruction %apricot_runOn with the node list created previously."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%apricot_runOn $clusterName $nodelist sudo apt install --yes python python-dev python3-pip swig python-nibabel python-six python-numpy python3-numpy mpich"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, install dependeces on the frontend node"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%apricot exec $clusterName sudo apt install --yes mpich python python-dev swig python3-pip python-numpy python3-numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%apricot exec $clusterName pip install --user numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%apricot exec $clusterName pip3 install --user mpi4py numpy nibabel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code compilation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point our frontend and working nodes have all the required dependencies, so is time to upload our program source code. This can easily be done using the %apricot_upload instruction,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%apricot_upload $clusterName CAIF.dat PyWrapper.tar.xz fitPerfusion.py $rootTarName /home/$username"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also, we need to download the input image from our public S3 bucket using the following instruction,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%apricot exec $clusterName curl https://grycap.s3.amazonaws.com/datasets/apricot/MRI/target.tar.xz --output target.tar.xz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The image is not stored at the github repository because its size. Now, all required data and files are at the cluster. Extract the tar files to compile our code,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%apricot exec $clusterName tar -xf PyWrapper.tar.xz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%apricot exec $clusterName tar -xf $rootTarName --directory PyWrapper/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extract DICOM image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%apricot exec $clusterName tar -xf target.tar.xz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compile the code and create the python wrapper using the provided bash script \"createWrapper.sh\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%apricot exec $clusterName cd PyWrapper && bash createWrapper.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the compiled python wrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%apricot exec $clusterName cp PyWrapper/_perfusion.so PyWrapper/perfusion.py ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, if all the steps have been executed correctly, we should see the following files in the user home directory,\n",
    "\n",
    "CAIF.dat  \n",
    "fitPerfusion.py  \n",
    "perfusion.py  \n",
    "\\_perfusion.so  \n",
    "PyWrapper  \n",
    "PyWrapper.tar.xz  \n",
    "root_v6.16.00.Linux-ubuntu16-x86_64-gcc5.4.tar.gz  (or the name introduced at the varaible \"rootTarName\")  \n",
    "target  \n",
    "target.tar.xz  \n",
    "\n",
    "Execute a \"ls\" to check generated files,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%apricot exec $clusterName ls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Execution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point, both node types, frontend and workers, are ready for the execution. So, lets run our program \"fitPerfusion.py\" with the instruction %apricot_MPI. Notice that the program may report warnings when processes voxels with noise as signal. Notice also that the execution may take a few minutes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%apricot_MPI $clusterName 3 3 /home/ubuntu python3 /home/$username/fitPerfusion.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wait until the computation finish. The following instruction shows the actual job queue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%apricot exec $clusterName squeue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the fit has concluded, download the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%apricot_download $clusterName results.dat ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To visualize the results, first, read the downloaded file \"results.dat\" and extract the data of interest,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Read fit file\n",
    "import numpy as np\n",
    "\n",
    "with open(\"results.dat\") as file:  \n",
    "    data = file.read() \n",
    "    data = data.strip('\\n').split('\\n')\n",
    "\n",
    "    ktrans  = np.zeros(shape=(len(data)),dtype=np.float)\n",
    "    kep     = np.zeros(shape=(len(data)),dtype=np.float)\n",
    "    ve      = np.zeros(shape=(len(data)),dtype=np.float)\n",
    "    \n",
    "    cont = 0\n",
    "    for line in data:\n",
    "        words = line.strip().split()\n",
    "        ktrans[cont] = float(words[1])\n",
    "        kep[cont]    = float(words[0])\n",
    "        ve[cont]     = float(words[2])\n",
    "        cont = cont+1\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reshape the results for plotting purposes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ktrans = np.reshape(ktrans,(256,256,56))\n",
    "kep    = np.reshape(kep,(256,256,56))\n",
    "ve     = np.reshape(ve,(256,256,56))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot some $v_e$ planes from results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plane = 2\n",
    "\n",
    "plt.figure(figsize=(40,40))\n",
    "\n",
    "plt.subplot(2,2,1)\n",
    "plt.xticks([])\n",
    "plt.yticks([])\n",
    "\n",
    "plt.imshow(np.rot90(ve[:,:,plane]), cmap='gray', vmin = 0, vmax = 1)\n",
    "\n",
    "plt.subplot(2,2,2)\n",
    "plt.xticks([])\n",
    "plt.yticks([])\n",
    "\n",
    "plt.imshow(np.rot90(ve[:,:,plane+10]), cmap='gray', vmin = 0, vmax = 1)\n",
    "\n",
    "plt.subplot(2,2,3)\n",
    "plt.xticks([])\n",
    "plt.yticks([])\n",
    "\n",
    "plt.imshow(np.rot90(ve[:,:,plane+20]), cmap='gray', vmin = 0, vmax = 1)\n",
    "\n",
    "plt.subplot(2,2,4)\n",
    "plt.xticks([])\n",
    "plt.yticks([])\n",
    "\n",
    "plt.imshow(np.rot90(ve[:,:,plane+30]), cmap='gray', vmin = 0, vmax = 1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Free resources"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, once our experimentation has been concluded, free the cluster resources using %apricot destroy instruction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%apricot destroy $clusterName"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this notebook, we have fitted a real MRI image using a perfusion model to obtain the transfer coefficient between blood plasma and the compartment, and the extracellular extravascular fractional volume. This results are fully reproducible using this notebook and some supported cloud provider to deploy the required infrastructure."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
